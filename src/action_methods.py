import os

os.environ["PYSPARK_PYTHON"] = r"C:\Users\Mohan\AppData\Local\Programs\Python\Python311\python.exe"
os.environ["PYSPARK_DRIVER_PYTHON"] = r"C:\Users\Mohan\AppData\Local\Programs\Python\Python311\python.exe"

from pyspark.sql import SparkSession

# Create Spark session
spark = SparkSession.builder \
    .appName("Spark Actions Demo") \
    .master("local[*]") \
    .getOrCreate()

sc = spark.sparkContext

# -------------------------------------------------
# Create RDD
# -------------------------------------------------
rdd = sc.parallelize([1, 2, 3, 4, 5])

# -------------------------------------------------
# 1Ô∏è‚É£ collect()
# -------------------------------------------------
print("collect():", rdd.collect())

# Output:
# collect(): [1, 2, 3, 4, 5]


# -------------------------------------------------
# 2Ô∏è‚É£ count()
# -------------------------------------------------
print("count():", rdd.count())

# Output:
# count(): 5


# -------------------------------------------------
# 3Ô∏è‚É£ take(n)
# -------------------------------------------------
print("take(3):", rdd.take(3))

# Output:
# take(3): [1, 2, 3]


# -------------------------------------------------
# 4Ô∏è‚É£ first()
# -------------------------------------------------
print("first():", rdd.first())

# Output:
# first(): 1


# -------------------------------------------------
# 5Ô∏è‚É£ reduce()
# -------------------------------------------------
sum_value = rdd.reduce(lambda x, y: x + y)
print("reduce():", sum_value)

# Output:
# reduce(): 15


# -------------------------------------------------
# 6Ô∏è‚É£ foreach()
# -------------------------------------------------
print("foreach() output:")
rdd.foreach(lambda x: print(x))

# Output (printed from executors):
# 1
# 2
# 3
# 4
# 5


# -------------------------------------------------
# 7Ô∏è‚É£ saveAsTextFile()
# -------------------------------------------------
rdd.saveAsTextFile("output_folder")

# Output:
# Folder created ‚Üí output_folder/
# Contains part files with data


# -------------------------------------------------
# Create DataFrame
# -------------------------------------------------
data = [
    ("Alice", 21),
    ("Bob", 22),
    ("David", 23)
]

df = spark.createDataFrame(data, ["name", "Age"])

# -------------------------------------------------
# 8Ô∏è‚É£ show()
# -------------------------------------------------
df.show()

# Output:
# +-----+---+
# | name|Age|
# +-----+---+
# |Alice|21 |
# | Bob |22 |
# |David|23 |
# +-----+---+


# -------------------------------------------------
# 9Ô∏è‚É£ count() on DataFrame
# -------------------------------------------------
print("DataFrame count():", df.count())

# Output:
# DataFrame count(): 3


# -------------------------------------------------
# üîü collect() on DataFrame
# -------------------------------------------------
print("DataFrame collect():", df.collect())

# Output:
# [Row(name='Alice', Age=21),
#  Row(name='Bob', Age=22),
#  Row(name='David', Age=23)]


# -------------------------------------------------
# 1Ô∏è‚É£1Ô∏è‚É£ head()
# -------------------------------------------------
print("head():", df.head())

# Output:
# Row(name='Alice', Age=21)


# -------------------------------------------------
# 1Ô∏è‚É£2Ô∏è‚É£ write()
# -------------------------------------------------
df.write.csv("df_output")

# Output:
# Folder created ‚Üí df_output/
# Contains CSV part files


# Keep Spark running for Spark UI
input("Press Enter to stop Spark...")